# Natural Language Processing - Distributed Methods

This project aims to conduct distributed data cleaning and wrangling processing on a huge dataset (the final dataset is comprised of 100+ million tweets). This set of codes can be leveraged and scaled to any project that involves memory-intensive tasks.

I crawled this dataset from scratch using the new Twitter API v2 via the twarc package.
